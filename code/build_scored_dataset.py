import torch
import os
import json
import argparse
from datasets import load_dataset
from typing import List, Dict, Tuple, Set, Any
from transformers import AutoModelForCausalLM, AutoTokenizer, logging, AutoModelForSeq2SeqLM
from src.eval import initialize_models, judge
from tqdm import tqdm
from peft import PeftModel

logging.set_verbosity_error()

DEFAULT_DATASET_PATH = "theblackcat102/ADL_Final_25W_part1_with_cost"
DEFAULT_SAFETY_MODEL = "Qwen/Qwen3Guard-Gen-0.6B"
DEFAULT_USEFULNESS_MODEL = "theblackcat102/Qwen3-1.7B-Usefulness-Judge"
DEFAULT_CHAT_MODEL = "unsloth/Llama-3.2-3B-Instruct"
DEFAULT_REWRITE_MODEL = "google/flan-t5-large"       
DEFAULT_INPUT_SCORED = "data/scored_responses.jsonl"
DEFAULT_OUTPUT_SCORED = "data/scored_responses.jsonl"

def parse_args():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸ï¼Œè¨­å®šæ¨¡å‹å’Œè¨“ç·´é…ç½®ã€‚"""
    parser = argparse.ArgumentParser("Score Rewritten Prompts")

    parser.add_argument("--dataset", type=str, default=DEFAULT_DATASET_PATH,
                        help="åŸå§‹æç¤ºçš„è³‡æ–™é›†è·¯å¾‘ã€‚")
    parser.add_argument("--guard_model", type=str, default=DEFAULT_SAFETY_MODEL)
    parser.add_argument("--usefulness_model", type=str, default=DEFAULT_USEFULNESS_MODEL)
    parser.add_argument("--chat_model", type=str, default=DEFAULT_CHAT_MODEL)
    parser.add_argument("--rewrite_model", type=str, default=DEFAULT_REWRITE_MODEL) 
    parser.add_argument("--lora_path", type=str, default=None)
    # æ–°å¢ç¨ç«‹çš„è¼¸å…¥/è¼¸å‡ºåƒæ•¸ï¼Œæ»¿è¶³ä¸æ›´å‹•åŸæª”æ¡ˆçš„è¦æ±‚
    parser.add_argument("--input_scored", type=str, default=DEFAULT_INPUT_SCORED,
                        help="ç¾æœ‰çš„å–®ä¸€è©•åˆ†æ•¸æ“šæª”æ¡ˆè·¯å¾‘ (æ­¤æª”æ¡ˆä¸æœƒè¢«ä¿®æ”¹)ã€‚")
    parser.add_argument("--output_scored", type=str, default=DEFAULT_OUTPUT_SCORED,
                        help="è¼¸å‡ºåŒ…å«èˆŠè³‡æ–™å’Œæ–°ç”Ÿæˆè³‡æ–™çš„å®Œæ•´æª”æ¡ˆè·¯å¾‘ (æ­¤æª”æ¡ˆæœƒè¢«å‰µå»ºæˆ–è¦†è“‹)ã€‚")
                        
    parser.add_argument("--iterations", type=int, default=10,
                        help="æ¯å€‹åŸå§‹æç¤ºç”Ÿæˆå’Œè©•åˆ†çš„æ¬¡æ•¸ã€‚")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="é™åˆ¶è™•ç†çš„åŸå§‹æç¤ºæ•¸é‡ï¼Œç”¨æ–¼èª¿è©¦ã€‚")
    return parser.parse_args()


def load_original_dataset(path: str):
    """è¼‰å…¥åŸå§‹æç¤ºè³‡æ–™é›†ã€‚"""
    print(f"Loading dataset {path} ...")
    ds = load_dataset(path)
    split = list(ds.keys())[0]
    ds = ds[split]
    if 'prompt' not in ds.column_names:
        raise ValueError("Dataset must contain column 'prompt'")
    return ds

def load_existing_data(file_path: str) -> Tuple[List[Dict[str, Any]], Set[Tuple[str, str]]]:
    """
    è¼‰å…¥éå¾€ç”Ÿæˆçš„æ•¸æ“šï¼Œä¸¦å»ºç«‹ä¸€å€‹é›†åˆç”¨æ–¼å¿«é€Ÿå»é‡ã€‚
    """
    existing_data: List[Dict[str, Any]] = []
    dedup_set: Set[Tuple[str, str]] = set()
    
    # ç¬¬ä¸€æ¬¡é‹è¡Œæ™‚ï¼Œæª”æ¡ˆå¯èƒ½ä¸å­˜åœ¨ï¼Œé€™æ˜¯æ­£å¸¸çš„
    if not os.path.exists(file_path):
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}ï¼Œå°‡å¾é›¶é–‹å§‹ç”Ÿæˆã€‚")
        return existing_data, dedup_set

    print(f"--- æ­¥é©Ÿ 1: è®€å–ç¾æœ‰è©•åˆ†æ•¸æ“š {file_path} (ä¿æŒåŸæª”æ¡ˆä¸è®Š) ---")
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, desc="Loading Existing Data"):
            try:
                item = json.loads(line)
                if all(k in item for k in ['prompt', 'response', 'reward']):
                    existing_data.append(item)
                    dedup_set.add((item['prompt'], item['response']))
            except json.JSONDecodeError:
                continue
    
    print(f"å·²è¼‰å…¥ {len(existing_data)} ç­†ç¾æœ‰è¨˜éŒ„ï¼Œå…¶ä¸­åŒ…å« {len(dedup_set)} å€‹å”¯ä¸€çµ„åˆã€‚")
    return existing_data, dedup_set

def run_rewrite_model(prompt: str, model, tokenizer, max_new_tokens=512) -> str:
    """ç”¨ Seq2Seq æ¨¡å‹é‡å¯«æç¤ºã€‚"""
    instruction_prefix = "TASK: Rewrite the unsafe INPUT prompt below. The goal is to transform the potentially harmful, malicious, or unsafe input into a less harmful and safe query. The modified query MUST strictly retain the user's original core goal and intent, changing only the elements necessary for safety. Prioritize minimizing the deviation from the original context. Do not add external context or introduce new goals. The output must be the modified prompt directly and nothing else. INPUT: "
    input_text = instruction_prefix + prompt
    
    encoded = tokenizer(input_text, max_length=512, truncation=True, padding="max_length", return_tensors="pt")

    outputs = model.generate(
        input_ids=encoded["input_ids"].to(model.device),
        attention_mask=encoded["attention_mask"].to(model.device),
        max_length=max_new_tokens,
        do_sample=True,
        top_p=0.8,
        temperature=0.9,
        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    rewritten_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return rewritten_text.strip()


def score_and_record(prompt: str, rewritten: str, iteration_idx: int, sample_id: int) -> Dict:
    """ä½¿ç”¨ judge() è©•åˆ†ï¼Œä¸¦æ ¼å¼åŒ–çµæœã€‚"""
    eval_result = judge(rewritten, prompt) 

    reward = float(eval_result["safety_score"]) * float(eval_result["relevance_score"])

    return {
        "id": sample_id,          
        "iteration": iteration_idx, 
        "prompt": prompt,
        "response": rewritten,
        "reward": reward,
        "safety": float(eval_result["safety_score"]),
        "relevance": float(eval_result["relevance_score"])
    }


def main():
    args = parse_args()

    # 1. è¼‰å…¥éå¾€æ•¸æ“šä¸¦å»ºç«‹å»é‡é›†åˆ (å¾æŒ‡å®šçš„ input æª”æ¡ˆè®€å–)
    existing_data, dedup_set = load_existing_data(args.input_scored)

    # 2. åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹
    print("--- æ­¥é©Ÿ 2: åˆå§‹åŒ–è©•ä¼°èˆ‡é‡å¯«æ¨¡å‹ ---")
    initialize_models(args.guard_model, args.usefulness_model, args.chat_model)
    
    rewrite_model = AutoModelForSeq2SeqLM.from_pretrained(
        args.rewrite_model,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )
    rewrite_tokenizer = AutoTokenizer.from_pretrained(args.rewrite_model)
    if args.lora_path is not None:
        rewrite_model = PeftModel.from_pretrained(rewrite_model, args.lora_path)
    # 3. è¼‰å…¥åŸå§‹è³‡æ–™é›†
    ds = load_original_dataset(args.dataset)

    if args.max_samples is not None:
        ds = ds.select(range(args.max_samples))

    newly_generated_data: List[Dict] = [] 
    
    print("--- æ­¥é©Ÿ 3: ç”Ÿæˆã€å»é‡ä¸¦è©•åˆ† (Scoring) ---")
    pbar = tqdm(total=len(ds)*args.iterations, ncols=0)
    # 4. ç”Ÿæˆã€å»é‡å’Œè©•åˆ†å¾ªç’°
    for idx, row in enumerate(ds):
        prompt = row["prompt"]
        
        for it in range(args.iterations):
            rewritten = run_rewrite_model(prompt, rewrite_model, rewrite_tokenizer)
            dedup_key = (prompt, rewritten)
            # å»é‡æª¢æŸ¥
            if dedup_key in dedup_set:
                pbar.update(1)
                continue 
            
            # è©•åˆ†ä¸¦è¨˜éŒ„æ–°çš„æ•¸æ“š
            result = score_and_record(prompt, rewritten, it, idx)
            newly_generated_data.append(result)
            
            # å°‡æ–°ç”Ÿæˆçš„æ•¸æ“šåŠ å…¥å»é‡é›†åˆï¼Œä»¥é˜²åœ¨åŒä¸€è¼ªä¸­é‡è¤‡
            dedup_set.add(dedup_key)
            pbar.update(1)
    print(f"\nâœ… å®Œæˆç”Ÿæˆã€‚æœ¬æ¬¡æ–°å¢äº† {len(newly_generated_data)} å€‹æ–°çš„è©•åˆ†æ¨£æœ¬ã€‚")
    
    # 5. åˆä½µä¸¦å„²å­˜çµæœ (å¯«å…¥åˆ°æ–°çš„ output æª”æ¡ˆ)
    
    output_path = args.output_scored
    os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
    
    # çµåˆèˆŠæ•¸æ“šå’Œæ–°æ•¸æ“š
    final_data = existing_data + newly_generated_data
    final_data.sort(key=lambda x: x['id']) 
    print(f"--- æ­¥é©Ÿ 4: å„²å­˜å®Œæ•´æ•¸æ“š ({len(final_data)} ç­†è¨˜éŒ„) åˆ° {output_path} ---")

    # ä½¿ç”¨ 'w' æ¨¡å¼ (Write/Overwrite)
    with open(output_path, "w", encoding="utf-8") as f:
        for record in tqdm(final_data):
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(f"\nğŸ”¥ è©•åˆ†è…³æœ¬é‹è¡Œå®Œæˆã€‚åŸæª”æ¡ˆ {args.input_scored} æœªæ›´å‹•ã€‚")
    print(f"ä¸‹ä¸€éšæ®µï¼šé‹è¡Œ create_preference_pairs.py å°‡ {output_path} è½‰æ›ç‚ºåå¥½å°ã€‚")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nè‡´å‘½éŒ¯èª¤: {e}")
        print("è«‹æª¢æŸ¥æ‚¨çš„ç’°å¢ƒè¨­ç½®å’Œ 'src.eval' æ¨¡çµ„æ˜¯å¦æ­£ç¢ºå®šç¾©ã€‚") 
